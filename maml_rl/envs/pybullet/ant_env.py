
from pybullet_envs.gym_locomotion_envs import AntBulletEnv
import numpy as np
        
class AntDirEnv(AntBulletEnv):
    """
    ant bullet env with fwd back
    forward: x=1e3, y=0
    backword: x=-1e3, y=0
    """
    def __init__(self, task={}):
        super(AntDirEnv, self).__init__()
        self._task = task
        self.max = 1e3
        self.min = -1e3
        self.walk_target_x, self.walk_target_y = task.get('direction', (self.max,0.0))
        self.robot.walk_target_x, self.robot.walk_target_y = task.get('direction', (self.max,0.0))

    def sample_tasks(self, unseen_tasks, num_tasks):
        directions = (2 * self.np_random.binomial(1, p=0.5, size=(num_tasks,)) - 1) * self.max
        tasks = [{'direction': (direction,0.0)} for direction in directions]
        return tasks

    def sample_unseen_task(self, tasks):
        #dir has no unsee task so return fwsd or bck
        direction = (2 * self.np_random.binomial(1, p=0.5) - 1) * self.max
        unseen_task = [{'direction': (direction,0.0)}]
        return unseen_task

    def reset_task(self, task):
        self._task = task
        self.walk_target_x, self.walk_target_y = task['direction']
        self.robot.walk_target_x, self.robot.walk_target_y = task['direction']


class AntVelEnv(AntBulletEnv):
    """
    The ant follows the dynamics from MuJoCo [2], and receives at each 
    time step a reward composed of a control cost and a penalty equal to the 
    difference between its current velocity and the target velocity. The tasks 
    are generated by sampling the target velocities from the uniform 
    distribution on [0, 2].
    """
    def __init__(self, task={}):
        super(AntVelEnv, self).__init__()
        self._task = task
        self._goal_vel = task.get('velocity', 0.0)
        self.max = 2.0
        self.min = 0.0

    def sample_tasks(self, unseen_tasks, num_tasks):
        velocities = self.np_random.uniform(self.min, self.max, size=(num_tasks,))
        unseen = [vel['velocity'] for vel in unseen_tasks]
        for i in range(velocities.shape[0]):
            while velocities[i] in unseen:
                velocities[i] = self.np_random.uniform(self.min, self.max)
        tasks = [{'velocity': velocity} for velocity in velocities]
        return tasks
    
    def sample_unseen_task(self, tasks):
        velocity = self.np_random.uniform(self.min, self.max)
        velocities = [vel['velocity'] for vel in tasks]
        while velocity in velocities:
            velocity = self.np_random.uniform(self.min, self.max)
        unseen_task = [{'velocity': velocity}]
        return unseen_task


    def reset_task(self, task):
        self._task = task
        self._goal_vel = task['velocity']

    def step(self, a):
        if not self.scene.multiplayer:  # if multiplayer, action first applied to all robots, then global step() called, then _step() for all robots with the same actions
            xposbefore = self.robot.body_xyz[0]
            self.robot.apply_action(a)
            self.scene.global_step()

        state = self.robot.calc_state()  # also calculates self.joints_at_limit
        self._alive = float(self.robot.alive_bonus(state[0] + self.robot.initial_z,self.robot.body_rpy[1]))  # state[0] is body height above ground, body_rpy[1] is pitch
        done = self._isDone()
        if not np.isfinite(state).all():
            print("~INF~", state)
            done = True

        forward_vel = (self.robot.body_xyz[0] - xposbefore) / self.robot.scene.dt
        forward_reward = -1.0 * abs(forward_vel - self._goal_vel) + 1.0
        survive_reward = 0.05
        ctrl_cost = 0.5 * 1e-2 * np.sum(np.square(a))
        contact_cost = 0.5 * 1e-3 * self._alive
        reward = forward_reward - ctrl_cost - contact_cost + survive_reward
        self.reward += reward
  
        self.HUD(state, a, done)

        return state, reward, bool(done), {}


class AntPosEnv(AntBulletEnv):
    """
       The ant follows the dynamics from MuJoCo [1], and receives at each 
    time step a reward composed of a control cost, a contact cost, a survival 
    reward, and a penalty equal to its L1 distance to the target position. The 
    tasks are generated by sampling the target positions from the uniform 
    distribution on [-3, 3]^2.
    """
    def __init__(self, task={}):
        super(AntPosEnv, self).__init__()
        self._task = task
        self.walk_target_x, self.walk_target_y = task.get('position', np.zeros((2,), dtype=np.float32))
        self.robot.walk_target_x, self.robot.walk_target_y = task.get('position', np.zeros((2,), dtype=np.float32))
        self.max = 1e3
        self.min = -1e3

    #sample task each batch
    def sample_tasks(self, unseen_tasks, num_tasks):
        positions = self.np_random.uniform(self.min, self.max, size=(num_tasks, 2)) 
        unseen = [pos['position'] for pos in unseen_tasks]
        for i in range(positions.shape[0]):
            while (positions[i] == unseen).all(1).any():
                positions[i] = self.np_random.uniform(self.min, self.max, 2)
        tasks = [{'position': position} for position in positions]
        return tasks


    #sample tasks
    def sample_unseen_task(self, tasks):
        position = self.np_random.uniform(self.min, self.max, 2)
        positions = [pos['position'].tolist() for pos in tasks]
        while (position.tolist() in positions):
            position = self.np_random.uniform(self.min, self.max, 2)
        unseen_task = [{'position': position}]
        return unseen_task

    def reset_task(self, task):
        self._task = task
        self.walk_target_x, self.walk_target_y = task['position']
        self.robot.walk_target_x, self.robot.walk_target_y  = task['position']
